# TP Kubernetes

# Info Pratique:
adresse email intervenant: amnay.m@gmail.com

# Preparation

- Disposer d'un compte [Google](https://accounts.google.com/signup) (Si vous avez une boite mail GMail, c'est tout bon!)
- Disposer d'un compte [Github](https://github.com/signup)
- Disposer d'un compte [Docker Hub](https://hub.docker.com/signup)
- **important** renseigner vos informations sur ce [google sheet](https://docs.google.com/spreadsheets/d/13IHlEyLHt_dNwGujlVxwRdqYJ65BRv-kGvx5GZJJ7qY/edit?usp=sharing)

# üéâüéâ  Dockerfile + Code python du microservice [ici](https://gist.github.com/amnay-mo/decf9807eec917b2c677f8c88c256ea8)  üéâüéâ

# TP 1

Ecrire un microservice http "motd-api" (MOTD == Message of the Day)
- Le microservice retourne toujours un objet json `{"message": MESSAGE}`
- le MESSAGE doit √™tre configurable
- Le service prend deux variables d'environnement pour sa configuration:
  - `MESSAGE` est le message retourn√© par l'API
  - `APP_PORT` est le port d'√©coute de l'API


```
‚ûú  motd-api git:(master) ‚úó APP_PORT=5555 MESSAGE="Hello World!" ./motd-api
2021/03/22 10:17:21 starting app
2021/03/22 10:17:41 127.0.0.1:42306 GET /
```

```
‚ûú  ~ http GET ':5555/'
HTTP/1.1 200 OK
Content-Length: 27
Content-Type: application/json
Date: Mon, 22 Mar 2021 09:17:41 GMT

{
    "message": "Hello World!"
}
```

- pousser votre code dans votre d√©pot git
- invitez-moi dans votre depot git; mon pseudo github : "amnay-mo"


# TP 2 

- Dockeriser "motd-api"
- Fournir le Dockerfile dans le d√©pot git

# TP 3

- Pousser l'image Docker dans votre docker hub


```sh
$ docker login # il faudra vous authentifier au moins une fois
$ docker build -t <COMPTE_DOCKER_HUB>/<NOM_IMAGE_DOCKER>:<VERSION> .
$ docker push  <COMPTE_DOCKER_HUB>/<NOM_IMAGE_DOCKER>:<VERSION>
```

- Documenter le nom de l'image dans le README du d√©pot GIT


# TP 4 : installation de gcloud et kubectl

‚ö†Ô∏è **Utiliser l'adresse gmail que vous m'avez donn√© pour l'authentification GCloud**

- installer l'outil CLI GCLOUD: https://cloud.google.com/sdk/docs/install
- t√©l√©chargez le client kubernetes `kubectl` et ajoutez-le dans votre path:

```sh
https://dl.k8s.io/release/v1.24.9/bin/windows/amd64/kubectl.exe
```

- V√©rifiez que kubectl est bien install√©:

```sh
kubectl version
```

- Bonus: activez l'auto-compl√©tion bash pour kubectl:

```sh
echo 'source <(kubectl completion bash)' >>~/.bashrc
```

‚ÑπÔ∏è **Kubectl** est votre compagnon pour toute interaction avec un cluster K8S. Voici un Cheat Sheet qui peut vous √™tre utile: https://kubernetes.io/docs/reference/kubectl/cheatsheet/

# TP 5 : setup de gcloud et kubectl

## Configuration de GCloud

- authentification:

```sh
gcloud auth login
```

- notre project google cloud s'appelle "amnay-k8s-337617"

```sh
gcloud config set project amnay-k8s-337617
```

- Lister le cluster kubernetes

```sh
gcloud container clusters list
```

- R√©cup√©rer les credentials de connexion au Cluster Kubernetes

```sh
export USE_GKE_GCLOUD_AUTH_PLUGIN=True
gcloud container clusters get-credentials --zone=europe-west9-b tp-k8s
```

## Kubectl

- V√©rifez que vous avez bien un nouveau context k8s:

```sh
kubectl config get-contexts
```

- V√©rifier qu'on arrive √† requeter  l'API du cluster kubernetes, en listant les `nodes` par exemple:

```sh
kubectl get nodes
```

# TP 6 : cr√©ation et configuration de votre namespace

- cr√©er son propre <namespace> sous forme de <prenom.nom>

```sh
kubectl create ns <prenom-nom>
```

- configurer le Namespace par d√©faut

```sh
kubectl config set-context --current --namespace=<prenom-nom>
```

‚ö†Ô∏è Veillez √† bien rester dans votre **namespace personnel** tout le long du TP

- V√©rifiez que votre `context` est correctement configur√©, avec le bon nom de `namespace`
- Vous verrez s'afficher pas mal d'informations utiles concernant votre cluster k8s, notamment l'URL publique du kube-apiserver

```sh
kubectl config view --minify
```

- V√©rifiez que le client `kubectl` arrive bien √† communiquer avec le serveur API du cluster Kubernetes:
- On arrive bien √† r√©cup√©rer la version du serveur API:

```sh
kubectl version
```

- On arrive bien √† lister les `nodes` du cluster:

```sh
kubectl get nodes
```

# TP 8 : premier pod

- Creer un fichier `pod.yml`:

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: <nom_de_votre_pod>
  labels:
    app: motd
spec:
  containers:
  - name: motd
    image: <nom_de_votre_image_docker>
```

- Faites √©couter le pod sur le port `80`

```sh
kubectl apply -f pod.yml
```

‚ö†Ô∏è **votre_image** doit √™tre accessible via un `docker pull` pour que Kubernetes puisse la r√©cup√©rer!

- Essayez d'appeler votre webservice apr√®s l'avoir rendu accessible √† l'aide de la commande

```sh
kubectl port-forward <nom_du_pod> <port_dans_votre_machine>:<port_du_pod>
```

- V√©rifiez que votre webservice √©crit bien des logs :
 
```sh
kubectl logs -f <nom_de_votre_pod>`
```

- Listez vos pods et observez les diff√©rents champs (le Status, le Node o√π il est d√©ploy√©):

```sh
kubectl get pods -o wide
```

- Vous pouvez aussi obtenir plus ample information sur votre pod avec la commande:

```sh
kubectl get pods -o yaml
```

- Editez votre pod ajouter un label suppl√©mentaire `env: prod` au pod

```sh
kubectl edit pod <nom_du_pod>
```

- supprimez ce pod

```sh
kubectl delete pod <nom_du_pod>
```

- v√©rifiez que votre pod a bien disparu

```sh
kubectl get pods
```

- recr√©ez-le

```sh
kubectl apply -f pod.yml
```

# TP 9 : Premier service

- Cr√©er un fichier `service.yml` qui d√©finit un service de type ClusterIP qui pointe sur le pod que vous avez cr√©√© dans l'√©tape pr√©cedente. utilisez cette doc pour vous guider (doc: https://kubernetes.io/docs/concepts/services-networking/service/)

- Appliquer le service sur le cluster:

```sh
kubectl apply -f service.yml
```

- V√©rifier que votre service a bien √©t√© cr√©√©:

```sh
kubectl get service
```

- Verifier que votre service pointe bien sur votre pod en listant les endpoints:

```sh
kubectl get endpoints
```

- Comme c'est un service de type `ClusterIP`, il n'est accessible que depuis l'int√©rieur du cluster
- Essayez de vous y connecter en lan√ßant en lan√ßant un curl depuis un pod de debug:

```sh
kubectl run -i --tty --rm debug --image=ubuntu --restart=Never -- bash
apt update && apt install -y httpie
http GET  'http://<nom-du-service>.<nom-du-namespace>.svc.cluster.local/'
```

- Faites  un nslookup sur le nom du service `<nom-du-service>.<nom-du-namespace>.svc.cluster.local`





```sh
apt update && apt install dnsutils
nslookup <nom-du-service>
```

- Essayez de requeter votre service en utilisant la version longue de son nom DNS

Tip: Si votre service n'est pas correctement connect√© √† votre pod, √©ditez le selecteur du Service et le Label du pod afin qu'ils correspondent:

```sh
kubectl edit svc <nom_du_service>
kubectl edit pod <nom_du_pod>
```

# TP 10 : Going Public!

- Une fois assur√©s que votre Service ClusterIP est fonctionnel, cr√©ez un service de type LoadBalancer et essayez d'y acc√©der en utilisant son adresse IP publique
üí°Il suffit d'ajouter le champ `type: LoadBalancer` √† la partie `spec` de la d√©finition de votre `Service`

# TP 11 : Deploiements multi-instances
- Supprimez votre pod
- V√©rifiez que votre service n'est plus disponible (via l'IP publique du service LoadBalancer)
- Cr√©er un "Deployment" qui d√©ploie 3 instances de votre conteneur motd
- Observez la cr√©ation de votre "Deployment" `kubectl get deploy -o wide`
- Gardez le m√™me Label que celui configur√© dans votre Service LoadBalancer (TP 10)
- V√©rifiez que votre service est toujours joignable
- V√©rifiez que vous avez bien pods en ex√©cution avec `kubectl get pods`
- Essayez de supprimer √† la main un des pods du d√©ploiement et observez que le pods manquant est aussit√¥t recr√©√©  `kubectl delete pod <nom_d_un_des_pods>`

Doc : https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
- Tip: Cr√©er un fichier yaml similaire √† celui de la doc, √©diter la config, puis appliquer le fichier avec `kubectl apply -f <fichier.yaml>`

## Config Map & Canary Deployment

- Cr√©er un fichier `configmap.yml` qui contient une configmap avec les cl√©/valeurs ( doc: https://kubernetes.io/docs/concepts/configuration/configmap ) 
  -  message: "Do not eat the yellow snow!"
-  Cr√©er un fichier `deployment_canary.yml` qui:
  -  utiliser la configmap cr√©√©e ci-dessus pour renseigner la variable d'env "MESSAGE" du container motd de ce deployment
  -  ce deployment √† aussi un label `app=motd` afin qu'il soit rout√© par le service LoadBalancer cr√©√© pr√©c√©demment
-  Acc√©dez √† votre service en faisant plusieurs requetes, et remarquez le round-robin effectu√© entre les deux deployments

# TP 12 : Ingress


- Un `NGINX-INGRESS-CONTROLLER` vient d'√™tre install√© dans le cluster Kubernetes. Son IP publique est consultable en jetant un oeil sur le service de type `LoadBalancer` dans namespace `nginx-ingress`
	- Plus besoin d'avoir chacun son propre service de type LoadBalancer pour exposer son service √† internet
- Supprimez donc tous les services de **votre namespace**
- Cr√©ez un service de type ClusterIP pour votre "Deployment"
- NB: si vous disposez de votre propre nom de domaine personnel , vous pouvez sauter cette √©tape
  - Demandez un nom de domaine √† votre instructeur (elle sera de la forme "prenom-nom.amnay.fr"
- Cr√©ez un `ingress` pour permettre l'acc√®s √† votre Service, en vous aidant de cette [doc](https://kubernetes.io/docs/concepts/services-networking/ingress/) ou [celle-ci](https://docs.nginx.com/nginx-ingress-controller/configuration/ingress-resources/basic-configuration/)
- Astuce: Le nom de domaine qui vous sera fourni sera renseign√© dans le champ `host` de votre ingress
- Astuce: assurez-vous d'ajouter la ligne `ingressClassName: nginx` dans la spec de votre `Ingress`
- V√©rifiez que vous arrivez √† acceder √† votre service via l'url `http://<prenom-nom>.amnay.fr/`

# TP 13: Grand nettoyage

- Cr√©√©z un r√©pertoire s'appelle `k8s` dans la racine de votre d√©p√¥t git
- Dans ce r√©pertoire, vous devez avoir les manifests k8s n√©cessaires au d√©ploiement de votre microservice:
  - le `Deployment` dans deployment.yml
  - le `Service` dans `service.yml`
  - la config du microservice dans un `ConfigMap` dans configmap.ym (les variables d'environnement de votre microservice doivent √™tre initialis√©es grace √† ce configmap)
  - le `Ingress` de votre service dans `ingress.yml`
- Commitez et poussez
- Supprimez votre namespace k8s `kubectl delete namespace <votre namespace>`
- Vous avez √† pr√©sent perdu toutes vos ressources sur k8s.
- Recr√©ez votre namespace `kubectl create namespace <votre namespace>`
- Red√©ployez votre microservice √† l'aide des manifests que vous avez cr√©√© dans `/k8s`
- V√©rifiez que votre microservice fonctionne correctement et est bien accessible depuis internet


# InitContainer

- Nous voulons √† pr√©sent ajouter un [InitContainer](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) au pod de notre application `motd`
- Le role de cet InitContainer est de cr√©er un fichier dans un volume du pod, et d'√©crire le "message" de `motd` dans ce fichier. doc concernant le type de volume √† utiliser pour cette exercice [ici](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/)
- Le plus simple serait de cr√©er une image docker d'un petit programme qui fait √ßa. Vous le poussez sur votre dockerhub et vous l'utilisez en tant que InitContainer dans votre pod
- Vous mettrez √† jour le code de l'application `motd`pour qu'il aille chercher le "message" dans ce fichier
- Si la lecture de ce fichier √©choue, `motd` doit √©crire une ligne de log pour signaler cet echec et utiliser la variable d'environnement "MESSAGE"
- Le message √©crit par le InitContainer doit provenir d'un [Secret](https://kubernetes.io/docs/concepts/configuration/secret/)

# StatefulSet

- Changer le code source de votre microservice `motd` de la mani√®re suivante:
  - il peut maintenant r√©cup√©rer le "message" depuis une base de donn√©e de votre choix
  - l'URL de la base de donn√©e est fournie par variable d'environnement
- Cr√©er un StatefulSet (avec une seule instance) et un service de type ClusterIP pour votre base de donn√©e. exemple de d√©finition de statefulset: https://gist.github.com/amnay-mo/ec02c6e6bbf4c6f41eaa8b233fa342d2 doc: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
- Remarquez qu'un PersistentVolumeClaim et un PersistentVolume ont √©t√© cr√©√©s

```sh
kubectl get pvc
kubectl get pv
```

- Remarquez aussi que Google Cloud a provisionn√© des disques qui correspondent √† votre PersistentVolume:

```sh
gcloud compute disks list
```

- Peuplez manuellement votre base de donn√©e avec au moins un message afin que votre application puisse l'utiliser
- Tester votre application
- Supprimez le pod (et non pas le statefulset) de votre base de donn√©e
- Remarquez qu'un nouveau pod est aussit√¥t cr√©√© par le StatefulSet, et que les donn√©es n'ont pas √©t√© perdues


# CI/CD : 

- Cr√©er un script "build-and-deploy.sh" qui permet de:
	- Build & Push l'image docker sur votre docker hub. La version du tag de l'image docker doit correspondre  aux 6 premiers caract√®res du hash du commit afin de g√©n√©rer un id de version unique pour chaque 
	- Mettre √† jour le **Deployment** avec l'image docker nouvellement cr√©√© dans votre cluster kubernetes (dans votre namespace bien-sur)
	
